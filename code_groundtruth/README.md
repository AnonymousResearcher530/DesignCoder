# üóÇÔ∏è Dataset Annotation: Human Refinement of CodeFun-Generated Frontend Code

This dataset documents a human-in-the-loop annotation process designed to improve the quality and structure of machine-generated frontend code. It includes **300 manually refined code samples** originally generated by **CodeFun**, one of the most widely used platforms for frontend code generation from design images.

## üë®‚Äçüíª Annotation Process

Two senior frontend developers independently annotated 300 UI code samples over the course of **2 weeks**. Each sample contained:

* A UI design mockup (visual screenshot)
* Corresponding code generated by CodeFun (HTML/CSS/JSX)

The annotation followed a two-stage protocol:

### 1. üîß Visual Consistency Correction

The first step aimed to **resolve discrepancies between the rendered code and the design image**, including:

* **Spacing issues**: incorrect padding, margin, or gap settings
* **Alignment errors**: misaligned text, images, icons
* **Style mismatches**: incorrect font size, color, border, background styles
* **Component loss**: missing **icons**, **background images**, or decorative elements
* **Layout breaks**: improper nesting or wrapping causing layout collapse

All visual corrections ensured the rendered result matched the original mockup as precisely as possible.

### 2. üß± Structural Refactoring for Maintainability

After visual alignment, annotators refactored the code to improve readability, reusability, and responsiveness:

* **Component grouping**: Merged code blocks that semantically belong to the same UI component
* **Containerization**: Introduced responsive layout containers (e.g., `flex`, `grid`)
* **Semantic structure**: Refined the hierarchy to follow modern best practices (e.g., using semantic tags, avoiding deep nesting)

> **Note:** The HTML and CSS files in this dataset were **built from original JSX and CSS code**. This conversion step ensures that rendering and structure are preserved in a static format, enabling consistent analysis.

## üßë‚Äçü§ù‚Äçüßë Expert Agreement Protocol

To ensure annotation consistency, the two annotators followed a strict consensus protocol:

* Annotated samples **independently** first
* Compared and discussed any differences
* Reached **mutual agreement** before finalizing
* Maintained a **shared annotation guideline document** throughout the process

### üß© Common Annotation Disagreements and Resolution Measures

| Disagreement Type                                          | Description                                                                                         | Resolution Measure                                                                                 |
| ---------------------------------------------------------- | --------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |
| 1. **Whether a visual difference requires correction**     | One annotator may see a small spacing or font size deviation as tolerable, the other does not       | Defined visual deviation thresholds (e.g., >2px spacing, >5% font size diff) that require action   |
| 2. **Whether missing components should be reconstructed**  | One annotator might ignore a missing icon or background image; the other insists on adding it       | Agreed to **always restore missing visual components** if present in the original design           |
| 3. **How to group elements into components**               | Different interpretations of what constitutes a "component" (e.g., label + input, or card + button) | Established semantic rules: if elements serve a single user intent, group them into one component  |
| 4. **How to structure responsive containers**              | One prefers `flex`; the other prefers `grid` or media queries                                       | Agreed to use `flex` as default, and only use `grid` when layout clearly aligns to 2D structure    |
| 5. **Whether to preserve machine-generated code patterns** | One favors rewriting from scratch; the other prefers minimal change to keep consistency             | Agreed to **minimize changes unless the original code violates readability or structural clarity** |

---

## ‚è±Ô∏è Time Investment

* **Total annotation time**: 2 weeks
* **Average time per sample**: 20 minutes
* **Total annotators**: 2 expert frontend engineers

## üßæ Dataset Summary

| Attribute         | Value                                         |
| ----------------- | --------------------------------------------- |
| Number of samples | 300                                           |
| Original source   | CodeFun-generated HTML/CSS/JSX                |
| Annotators        | 2 senior frontend developers                  |
| Annotation goals  | Visual fidelity + Code structure              |
| Protocol          | Independent annotation + consensus discussion |

## üìå Use Cases

This dataset is valuable for:

* Benchmarking UI Designs to code generation systems
* Evaluating visual-semantic alignment in design-to-code pipelines

